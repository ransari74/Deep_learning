{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing  Deep learning Models use Pytorch.nn.Moudule for beginners\n",
    "Here you will see how to build your own model using pytorch. We will start from the simplest to most complicated deep learning. There is a list of models we will cover in this tutorial. We only cover the programming here, if you are not familiar with the fundemetal of any of these you can easily search through internets.\n",
    "\n",
    "1. Regrassion\n",
    "2. Logistic Regression\n",
    "3. Customized Artificial Neural Network\n",
    "4. Recurrent Neural Network (Simple RNN, and LSTM)\n",
    "5. Convolution Neural Netwrk\n",
    "\n",
    "If you are familiar with Pytorch, you definitly know nn. Moudule Class enables programmer to build their customized model easily. You might think Keras is easier to work, but you will be suprised about the user firendly interface Pytorch is providing. In additon, when you can customised your model, you can build model combined two deep neural network together. For instance, think about predicting stock using RNN, you might think other features like weekday would effect the stock price, so, you can build model to combine classifer resutl with RNN result and make prediction!! Do you think is it amazing, let start learning how to build the Pytorch model as easy as Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Regression\n",
    "Regression is the simplest Artificial neural networks which transfers input_size=1 to output_size=1. To define the pytorch model, first you define the class name.\n",
    "Here we call it <b>LinearReg</b>. Then we define the <b>__init__</b> function with the inputs. Then, then just use <b>super(linearRegression, self).__init__()</b>.\n",
    "\n",
    "<img src='Image/Reg.PNG' align=\"center\"/> \n",
    "\n",
    "\n",
    "These coding are the same for building all models. Now, you can build all layer you want here using torch.nn. Here, the order of the layer does not matter, so, just add as much as layer you want. For the regression, We only need one Linear transformation. All the class you define, has two parts, first the <b>__init__</b> function which enables you to determine all the parameters and layers you want. Second, it is the <b>forward</b> function which makes relation amongs all layers you have defined. Finally, you built the linearRegression model.\n",
    "\n",
    "`Attention!! In this file we are building models, in the next tutorial we will learn how to build the training loop.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearReg(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearReg, self).__init__()\n",
    "        self.linear = nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearReg(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearReg(1,1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2035], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable  \n",
    "model(FloatTensor([4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Logistic Regression\n",
    "Logistic Regression is very similar to Regression. We just need to add one sigmoid function after linear transformation of the resutls. If you want just build your model and do not use `the built-in Crossentropy` function. The model would be like below. However, if you will use 'built-in Crosssentropy' function, you can use the simple Regression model instead of Logistic Regression. The reason is that, the built-in Crosssentropy function use the unactivated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticReg(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LogisticReg, self).__init__()\n",
    "        #layers\n",
    "        self.linear = nn.Linear(inputSize, outputSize)\n",
    "\n",
    "        \n",
    "        #activation functions\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticReg(\n",
       "  (linear): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticReg(3,4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4655, 0.4006, 0.6527, 0.3259],\n",
       "        [0.5602, 0.4533, 0.5481, 0.4358],\n",
       "        [0.5782, 0.4029, 0.5316, 0.4494],\n",
       "        [0.5937, 0.4574, 0.5036, 0.4898],\n",
       "        [0.4947, 0.4248, 0.6338, 0.3311]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample = torch.rand(5,3)\n",
    "model(Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Artificial Neural Network\n",
    "Neural Network(NN) is constructed from several layers. NN is one of the simplest application of deep learning. In deep learning model, we ask the model to discover the hidden features. In contrast, the machine learning algorithm the machine know the features. We can see one NN architecture below (image from google):\n",
    "\n",
    "<img src='https://miro.medium.com/max/3000/1*HWhBextdDSkxYvz0kEMTVg.png' />\n",
    "\n",
    "As it is clear, we have three layers with different activation function. So, as we explained it in the regression, we start to add as much as layer we want. The layers convert data from size= 784 (as input size)==> first hidden layer size=128 ==> second hidden layer size=64 ==> output size=10. In addition, you can add as much as layer you want in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(NN, self).__init__()\n",
    "        #Hidden layers\n",
    "        self.layer1 = nn.Linear(inputSize, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        \n",
    "        #output layer\n",
    "        self.layer3 = nn.Linear(64, outputSize)\n",
    "\n",
    "        \n",
    "        #activation functions\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NN(784,10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0989, 0.0938, 0.1007, 0.0976, 0.1053, 0.1091, 0.1055, 0.0919, 0.1007,\n",
       "         0.0965],\n",
       "        [0.0952, 0.0967, 0.1002, 0.0987, 0.1064, 0.1074, 0.1056, 0.0925, 0.0992,\n",
       "         0.0980],\n",
       "        [0.0943, 0.0948, 0.1052, 0.0984, 0.1044, 0.1089, 0.1034, 0.0910, 0.1013,\n",
       "         0.0983],\n",
       "        [0.0997, 0.0949, 0.1029, 0.0951, 0.1079, 0.1069, 0.1045, 0.0917, 0.1031,\n",
       "         0.0934],\n",
       "        [0.0966, 0.0947, 0.1021, 0.0974, 0.1028, 0.1110, 0.1028, 0.0945, 0.1018,\n",
       "         0.0963]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample = torch.rand(5,784)\n",
    "model(Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Recurrent Neural Network (RNN)\n",
    "Recurrent Neural Network algorithms are trying to capture information in a series of samples. They are used in text prediction, translation, voice recognition, and time series forecasting. There exists several models type based on the application of Recurrent neural network. Here, we will focus only on the simplest recurrent network application, time series, and will discuss on others (e.g. NLP) in another tutorial. There are three main structurs for Recurrent Neural Network cells:1.RNN, 2.GRU, 3.LSTM. Here, we will explain how to build RNN and LSTM using built-in pytorch function. The goals of RNN structur is to use several previous steps to predict the several steps ahead. So we defines problem as :\n",
    "\n",
    "<img src='Image/Time series problem defenition.PNG' />\n",
    "\n",
    "The idea behinds Recurrent Neural Networks is to send inframtion from learning of the first elemnet in Sequence to the the last one. This feature is shown below as hidden states. \n",
    "\n",
    "<img src='Image/RNN.PNG' />\n",
    "\n",
    "\n",
    "### Data input and output shape explanation\n",
    "\n",
    "For all types of Recurrent Neural Network model, the input is the 3 dimentional matrix with (Batch Size, Sequence Length, Number of features) and output is the 3 dimentional matrix with (Batch Size, Sequence Length, Hidden_size). First, we will implement the RNN using the built-in library. Then we will build RNN model from the scratch with nn.module. Inside each RNN layer, there are several RNN cells with the number of sequence lenght. Inside each RNN cells, there are several neurons which define the hidden_size. For example, in image below (soruce:google): Number of features is 3 which are shown with orange circle, and number of neuraons (hidden_size) is 3 which are shown with blue circles. In RNN, there is two groups input for each cell, first, the input data from trainging sets, second, the hidden states. So, we need to define an initial hidden state. \n",
    "\n",
    "\n",
    "<img src='Image/Inside_RNN_cell.png' />\n",
    "\n",
    "\n",
    "The oupput from RNN layer is the 3 dimentional matrix with (Batch Size, Sequence Length, Hidden_size). Based on the problem, we have to change this output. For example in time series forecasting, if we want to predict only the next time steps, we need an output shapes like (Batch Size, 1 , 1). So, we need to use the last output of the sequence, and using the linear layer to reduce Hidden_size to 1.  \n",
    "\n",
    "## simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layer):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layer\n",
    "\n",
    "        #Important Note: batch_first attribute means the first dimension in the Input data matrix is batch size and second one is the sequence length.\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layer, batch_first=True)   \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        print('The output shape of all cells in the  sequence:')\n",
    "        print(out.size())\n",
    "        # Choosing the last rnn cells output\n",
    "        out = out[:,-1,:]\n",
    "        print('The output shape of last cell in the sequence:')\n",
    "        print(out.size())\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(3, 10, batch_first=True)\n",
       "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RNN(3,1,10,1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output shape of all cells in the  sequence:\n",
      "torch.Size([40, 10, 10])\n",
      "The output shape of last cell in the sequence:\n",
      "torch.Size([40, 10])\n"
     ]
    }
   ],
   "source": [
    "Sample = torch.rand(40,10,3)\n",
    "out,hidden=model(Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "LSTM is a gated recurrent neural network. If you are not familiar wit the structure of LSTM, please search in through the internet. To implement the LSTM, we just need to add initial cell state along with the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layer):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layer\n",
    "\n",
    "        #Important Note: batch_first attribute means the first dimension in the Input data matrix is batch size and second one is the sequence length.\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layer, batch_first=True)   \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state\n",
    "        batch_size = x.size(0)\n",
    "        hidden, cell = self.init_hidden(batch_size)\n",
    "\n",
    "        \n",
    "        out, (hidden,cell) = self.lstm(x, (hidden,cell))\n",
    "        print('The output shape of all cells in the  sequence:')\n",
    "        print(out.size())\n",
    "        # Choosing the last rnn cells output\n",
    "        out = out[:,-1,:]\n",
    "        print('The output shape of last cell in the sequence:')\n",
    "        print(out.size())\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, (hidden,cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        #add this line for LSTM\n",
    "        cell = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        return (hidden,cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(3, 10, batch_first=True)\n",
       "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LSTM(3,1,10,1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output shape of all cells in the  sequence:\n",
      "torch.Size([40, 10, 10])\n",
      "The output shape of last cell in the sequence:\n",
      "torch.Size([40, 10])\n"
     ]
    }
   ],
   "source": [
    "Sample = torch.rand(40,10,3)\n",
    "out,(hidden,cell)=model(Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convoloution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the hyperparameters setting, and convolution dimension. Please visit https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "Here, we build the simple model using CNN with PyTroch. The architecture of the model is shown below:\n",
    "\n",
    "\n",
    "<img src='Image/CNN.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(384, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(x, (2, 2), stride=2)\n",
    "        print (x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output =F.relu(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0862, 0.3595, 0.2151, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320,\n",
       "         0.6520]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(1,1,10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
